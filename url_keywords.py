# -*- coding: utf-8 -*-
"""goodjobsfirst_expanded test_DOJ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q8zz3958t44Qna1Ho7TSTe8sSWz-N_TI

## Install requirements
"""

import pandas as pd
import requests
from datetime import datetime
from requests.adapters import HTTPAdapter
from bs4 import BeautifulSoup # For scraping links and text
import sys, argparse
import warnings
warnings.filterwarnings("ignore") 
# Warnings are important, but mostly not relevant here
# !pip install archivenow &>/dev/null; 
# For archiving pages to the Internet Archive's Wayback Machine
from archivenow import archivenow

def pages_from_baseline(baseline_links):
  # Find the first link we scraped during the previous run (this will be the most 
  # recently posted one and we will have scraped everything before it)
  firsts = baseline_links.drop_duplicates(subset="root", keep="first") 
  # Keep only these first links
  pages = {}
  for i, page in firsts.iterrows():
    pages[page['source']] = {}
    pages[page["source"]]["first_link"] = page["url"]
  return pages

def filter_new_links(all_links, baseline_links):
  new_links = list(set(all_links["full_url"]) - set(baseline_links["full_url"])) 
  # New links = links from later versions only
  all_links = all_links.loc[all_links["full_url"].isin(new_links)] 
  # Filter and redefine `all_links` as the new links
  return all_links

def get_links(soup, all_links, pages, page): 
  for link in soup:
    template = {"url": "", "source": "", "root": "", "full_url": "", 
                "keywords": "", "archive": "", "time_reviewed": ""}
    if link.has_attr('href'): # If the link is a URL...
      if link['href'] == pages[page]["first_link"]: 
        # Quit if we've gone back far enough - back to our baseline for this page
        raise ValueError('Found the first link we reviewed last time on ' + page)
      else:
        for pattern in pages[page]["ignore"]:
          # Weed out links we don't want
          if pattern in link['href']:
            pass
          else:
            for include in pages[page]["include"]: 
              # Keep only links that include this pattern
              if include in link['href']:
                template["url"] = link['href']
                template["source"] = page
                template["root"] = pages[page]["root"]
                template["full_url"] = pages[page]["root"] + link['href'] 
                template["keywords"] = pages[page]["keywords"]
                # This works unless it's an absolute rather than relative link....
                template["time_reviewed"] = datetime.today().strftime('%Y-%m-%d')
                # Insert actual date
                all_links.append(template)
              else:
                pass

def main(argv):
  parser = argparse.ArgumentParser(
        prog="`gjf1-1.py",
        description="Find occurrences of keywords in the urls that are input"
  )
  parser.add_argument(
        "-f",
        "--urls_csv",
        required=True,
        help="The file with pages (urls) to be processed"
  )
  group = parser.add_mutually_exclusive_group()
  group.add_argument("-b", "--baseline_csv",
                       required=False,
                       help="The baseline from a previous run. "
                       "Previously processed pages from this baseline will be skipped."
  ) 
  group.add_argument("-a", "--archive",
                     required=False,
                     help="Should pages be pushed to Internet Archive?")
  my_args = parser.parse_args()  

  
  """## List the pages we're interested in
  We can specify which links from these pages to ignore and/or include, based on 
  experience. For instance, we know that we can ignore URLs on `justice.gov` that 
  include `=facet_` and that we only want to include those that have `/pr/` in 
  the path.
  
  When we scrape a page like `https://www.justice.gov/usao/pressreleases` we will 
  go through `https://www.justice.gov/usao/pressreleases?page=1`, 
  `https://www.justice.gov/usao/pressreleases?page=2`, etc. The links we will 
  scrape are usually *relative* to a root domain. This means they are like 
  `/pr/title/subtitle`, which isn't a fully-specified URL. In combination with the 
  root domain, e.g. `https://www.justice.gov`, we get a full URL: 
  `https://www.justice.gov/pr/title/subtitle`
  """
  
  """https://www.attorneygeneral.gov/taking-action-archive": {"root": "https://www.attorneygeneral.gov", 
                                                          "ignore": ["/page/"], 
                                                          "include": ["/taking-action-archive/"], "first_link": ""},"""
  
  # Keys are pages to search, values are root domains that links are constructed from
  pages = {}
  if my_args.urls_csv:
    df = pd.read_csv(my_args.urls_csv)
    pages = df.set_index('source').T.to_dict()

  """## On later runs, load the "baseline" from last time the script was run, in 
  order to know when to stop scraping for new links
  WARNING: You won't be able to successfully run the following cell of code 
  without the `baseline_date.csv` file created from an initial or earlier run. 
  Do this only on later runs.
  """
  baseline_links = pd.DataFrame()
  if my_args.baseline_csv:
    baseline_links = pd.read_csv(my_args.baseline_csv)
    pages = pages_from_baseline(baseline_links)
  
  """## Scrape the press release pages of interest, collecting links
  Do this on both the initial and later runs.
  """
  
  all_links = []
  
  for page in pages.keys():
    # Initial review up to 20 search pages - default to ?page=20.
    for i in range(0,20): 
      # May need to change so that later runs go as high as needed to find 
      # first_link from previous search
      # Try to get links on this page
      try:
        s = requests.Session()
        s.mount("https://", HTTPAdapter(max_retries=3))
        # Some sites use "?page=2" for next pages, others use "/page/2"
        if i > 0:
          page_number = pages[page]["ignore"][0] + str(i)
          # page_number = "?page="+str(i)
        else:
          page_number = ""
        print("{} - {}".format(page, page_number))
        soup = BeautifulSoup(s.get(page+page_number).content, "html.parser").find_all("a") 
        # Get all links on the page
        get_links(soup, all_links, pages, page) 
      except ValueError as err:
        print(err)
        break
      except Exception as err:
        print(page, "Something went wrong on this page - {}".format(err))
  
  all_links = pd.DataFrame.from_dict(all_links)
  # print(all_links)
  
  """## Drop duplicates
  In scraping press release pages, we may often come across the same links on 
  different pages. These will usually be irrelevant anyway (they will be about 
  the site in general rather than specific press releases).
  """
  
  all_links = all_links.drop_duplicates(subset=["url", "source", "root"])
  # Full view:
  # all_links
  # Easy view:
  for i,x in all_links.iterrows():
      print(x["full_url"])
  
  all_links.to_csv("all_links.csv")
  
  """## Compare the current list of links to the previous one, filtering to new links
  **WARNING: Not relevant on the initial run and won't work**.
  
  This will show us which new press releases/pages have been posted since the 
  last time we ran the script, and help us focus our analysis.
  
  **In theory, we shouldn't need to do this if we are stopping our search at the 
  first link from last time - we should have *only* caught new links this time 
  around anyway.** But it's easy and good to do "just in case" and has the added 
  benefit of eliminating "noise" (irrelevant pages) not caught by the 
  ignore/include filters.
  """
  
  if my_args.baseline_csv:
      all_links = filter_new_links(all_links, baseline_links)
  
  """## Which pages should we actually manually review? Scrape their text, save 
  to the Wayback Machine, and count keywords
  This can be skipped on initial runs if we don't care about how much of a 
  "priority" they are.
  """
  
  failfile = "fail_{}.txt".format(datetime.today().strftime('%Y-%m-%d-%H:%M:%S')) 
  fp = open(failfile, "w")

  for index, page in all_links.iterrows():
    try:
      s = requests.Session()
      s.mount("https://", HTTPAdapter(max_retries=3))
      p = page["full_url"]
      print(p)
      # Scrape
      soup = BeautifulSoup(s.get(p).content, features="html.parser")
      body = soup.find('body')
      title = soup.find('title')
      keywords = page['keywords']
      for kw in keywords.split(","):
        kw = kw.strip()
        kw_body = "{}-body".format(kw)
        kw_title = "{}-title".format(kw)
        all_links.at[index,kw] = soup.get_text().lower().count(kw) 
        all_links.at[index,kw_body] = body.get_text().lower().count(kw) 
        all_links.at[index,kw_title] = title.get_text().lower().count(kw) 
        # get_text() is ok, but might count links that use the keywords...
      archive = None
      if my_args.archive:
        # Save to IA WM
        try:
          archive = archivenow.push(p,"ia")[0] # This can take a lot of time...
        except:
          pass
      all_links.at[index,"archive"] = archive
    except Exception as ex:
      print("{}- {}".format(p, ex), file=fp)
  
  fp.close()

  # all_links
  
  """## Add up total keyword counts and sort"""
  
  pd.set_option('max_colwidth', 1000) # Make the URL display bigger
  all_links["sum_body"] = 0
  all_links["sum_title"] = 0
  all_links["sum"] = 0
  for kw in keywords.split(","):
    kw = kw.strip()
    kw_body = "{}-body".format(kw)
    kw_title = "{}-title".format(kw)
    all_links["sum"] += all_links[kw]
    all_links["sum_body"] += all_links[kw_body]
    all_links["sum_title"] += all_links[kw_title]
  all_links["full_url"] = all_links["root"] + all_links["url"]
  all_links.sort_values(by="sum", ascending=False).head(50)[["full_url", "sum"]] 
  # Probably also should group by root (i.e. justice.gov's top links, TX AG's 
  # top links, etc.)
  
  """## Export list of links
  
  These will become the `baseline_links` for the next run. The CSV file will 
  appear in the "Files" panel on the left-hand side of the screen if run in 
  Google Colab.
  """
  
  outfile = "baseline_{}.csv".format(datetime.today().strftime('%Y-%m-%d-%H:%M:%S')) 
  all_links.to_csv(outfile)
  
def usage():
  print("Usage:  gjf1-1.py -f <pages_csv> [-b <baseline_csv>]") 
  exit


if __name__ == "__main__":
    if len(sys.argv) < 2:
        usage()
    else:
        main(sys.argv[1])


  
  